{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XaIo7E2c0mz",
        "outputId": "8153cbb1-430d-475d-d00c-d6735efe8b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cohere\n",
            "  Downloading cohere-5.5.8-py3-none-any.whl (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.8/173.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests (from transformers)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere)\n",
            "  Downloading boto3-1.34.137-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.21.2 (from cohere)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx-sse<0.5.0,>=0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.7.4)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20240622-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.2)\n",
            "Collecting botocore<1.35.0,>=1.34.137 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading botocore-1.34.137-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.21.2->cohere)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.21.2->cohere)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: xxhash, types-requests, requests, pyarrow, parameterized, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, httpx-sse, h11, fastavro, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, httpcore, botocore, s3transfer, nvidia-cusolver-cu12, httpx, datasets, boto3, sentence-transformers, cohere\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boto3-1.34.137 botocore-1.34.137 cohere-5.5.8 datasets-2.20.0 dill-0.3.8 fastavro-1.9.4 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 httpx-sse-0.4.0 jmespath-1.0.1 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 parameterized-0.9.0 pyarrow-16.1.0 requests-2.32.3 s3transfer-0.10.2 sentence-transformers-3.0.1 types-requests-2.32.0.20240622 xxhash-3.4.1\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-4.1.1-py3-none-any.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.2/216.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.6.2)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Installing collected packages: pinecone-plugin-interface, pinecone-client\n",
            "Successfully installed pinecone-client-4.1.1 pinecone-plugin-interface-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers sentence-transformers datasets cohere\n",
        "!pip install pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# API keys\n",
        "with open(\"chohere_api_keys.txt\") as f:\n",
        "    COHERE_API_KEY = f.read().strip()\n",
        "with open(\"pinecone_api_key.txt\") as f:\n",
        "    PINECONE_API_KEY = f.read().strip()"
      ],
      "metadata": {
        "id": "VPlyWRGpdBwj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import cohere\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import nltk\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import defaultdict\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Load embedding model\n",
        "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
        "model = SentenceTransformer(EMBEDDING_MODEL)\n",
        "\n",
        "# Function to split text into sentences\n",
        "def split_into_sentences(text):\n",
        "    return nltk.sent_tokenize(text)\n",
        "\n",
        "# Function to load and embed dataset with sentence chunking\n",
        "def load_and_embed_dataset(dataset_name: str, split: str, model: SentenceTransformer, text_field: str, rec_num: int) -> tuple:\n",
        "    print(\"Loading and embedding the dataset\")\n",
        "    dataset = load_dataset(dataset_name, split=split)\n",
        "    sentence_chunks = []\n",
        "    origin_docs = {}\n",
        "\n",
        "    for idx, doc in enumerate(dataset[text_field][:rec_num]):\n",
        "        sentences = split_into_sentences(doc)\n",
        "        sentence_chunks.extend(sentences)\n",
        "        for sentence in sentences:\n",
        "            normalized_sentence = sentence.strip().lower()\n",
        "            origin_docs[normalized_sentence] = idx  # Maintain the origin document index for each sentence\n",
        "\n",
        "    embeddings = model.encode(sentence_chunks)\n",
        "    print(\"Done!\")\n",
        "    return dataset, embeddings, sentence_chunks, origin_docs\n",
        "\n",
        "# Dataset parameters\n",
        "DATASET_NAME = 'ag_news'\n",
        "TEXT_FIELD = 'text'\n",
        "RECORDS_NUM = 200\n",
        "\n",
        "# Load and embed dataset\n",
        "dataset, embeddings, sentence_chunks, origin_docs = load_and_embed_dataset(\n",
        "    dataset_name=DATASET_NAME,\n",
        "    split='train',\n",
        "    model=model,\n",
        "    text_field=TEXT_FIELD,\n",
        "    rec_num=RECORDS_NUM\n",
        ")\n",
        "\n",
        "# Create Pinecone index\n",
        "def create_pinecone_index(index_name: str, dimension: int, metric: str = 'cosine'):\n",
        "    print(\"Creating a Pinecone index...\")\n",
        "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "    if index_name not in existing_indexes:\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=dimension,\n",
        "            metric=metric,\n",
        "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "        )\n",
        "    print(\"Done!\")\n",
        "    return pc\n",
        "\n",
        "INDEX_NAME = 'ag-news-index'\n",
        "pc = create_pinecone_index(INDEX_NAME, embeddings.shape[1])\n",
        "\n",
        "# Upsert vectors to Pinecone index\n",
        "def upsert_vectors(index: Pinecone, embeddings: np.ndarray, sentence_chunks: list, batch_size: int = 128):\n",
        "    print(\"Upserting the embeddings to the Pinecone index...\")\n",
        "    ids = [str(i) for i in range(embeddings.shape[0])]\n",
        "    meta = [{'text': text} for text in sentence_chunks]\n",
        "    to_upsert = list(zip(ids, embeddings, meta))\n",
        "    for i in tqdm(range(0, embeddings.shape[0], batch_size)):\n",
        "        i_end = min(i + batch_size, embeddings.shape[0])\n",
        "        index.upsert(vectors=to_upsert[i:i_end])\n",
        "    print(\"Done!\")\n",
        "    return index\n",
        "\n",
        "index = pc.Index(INDEX_NAME)\n",
        "index_upserted = upsert_vectors(index, embeddings, sentence_chunks)\n",
        "\n",
        "# Cohere client\n",
        "co = cohere.Client(api_key=COHERE_API_KEY)\n",
        "\n",
        "# Function to augment prompt with context based on origin documents\n",
        "def augment_prompt(query: str, model: SentenceTransformer, index=None, top_k: int = 3) -> str:\n",
        "    query_embedding = model.encode(query).tolist()\n",
        "    query_results = index.query(\n",
        "        vector=query_embedding,\n",
        "        top_k=top_k * 3,  # Retrieve more sentences to ensure we get top-k documents\n",
        "        include_values=True,\n",
        "        include_metadata=True\n",
        "    )['matches']\n",
        "\n",
        "    # Get the embeddings of the retrieved chunks\n",
        "    retrieved_embeddings = np.array([match['values'] for match in query_results])\n",
        "    retrieved_texts = [match['metadata']['text'].strip().lower() for match in query_results]\n",
        "\n",
        "    # Debugging: Print sentences that caused KeyError\n",
        "    for text in retrieved_texts:\n",
        "        if text not in origin_docs:\n",
        "            print(f\"Sentence causing KeyError: {text}\")\n",
        "\n",
        "    retrieved_doc_indices = [origin_docs.get(text, -1) for text in retrieved_texts]\n",
        "\n",
        "    # Compute cosine similarity between the query and the retrieved chunks\n",
        "    similarities = cosine_similarity([query_embedding], retrieved_embeddings)[0]\n",
        "\n",
        "    # Group sentences by their origin documents\n",
        "    doc_similarities = defaultdict(float)\n",
        "    doc_sentences = defaultdict(list)\n",
        "    for idx, doc_index in enumerate(retrieved_doc_indices):\n",
        "        if doc_index != -1:\n",
        "            doc_similarities[doc_index] += similarities[idx]\n",
        "            doc_sentences[doc_index].append(retrieved_texts[idx])\n",
        "\n",
        "    # Rank documents based on cumulative similarity\n",
        "    ranked_doc_indices = sorted(doc_similarities, key=doc_similarities.get, reverse=True)\n",
        "\n",
        "    # Select the top-ranked documents to use as context\n",
        "    selected_docs = ranked_doc_indices[:top_k]\n",
        "    source_knowledge = \"\\n\\n\".join([\" \".join(doc_sentences[doc_index]) for doc_index in selected_docs])\n",
        "\n",
        "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "    Contexts:\n",
        "    {source_knowledge}\n",
        "    If the answer is not included in the source knowledge - say that you don't know.\n",
        "    Query: {query}\"\"\"\n",
        "    return augmented_prompt, source_knowledge\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC-3q4p1dNKx",
        "outputId": "2fe42666-5b24-451b-e2c8-3dbdf0d3020a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and embedding the dataset\n",
            "Done!\n",
            "Creating a Pinecone index...\n",
            "Done!\n",
            "Upserting the embeddings to the Pinecone index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:02<00:00,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hallucinations_queries=[\"How much will Hewlett-Packard pay for Synstar?\",\n",
        "                      \"What did the scientists oppose in the Bush administration and how many of them were Nobel Prize winners?\",\n",
        "                      \"What was the annual base salary of Danny Bazil Riley when he started working as the general manager at a commercial real estate firm?\"\n",
        "                      ]"
      ],
      "metadata": {
        "id": "V41K_hR58VCM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1=hallucinations_queries[0]\n",
        "q2=hallucinations_queries[1]\n",
        "q3=hallucinations_queries[2]"
      ],
      "metadata": {
        "id": "72k9O_jSCRFv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q=q1\n",
        "print(\"-\" * 80)\n",
        "print(f\"Query: {q}\")\n",
        "print(\"-\" * 80)\n",
        "response = co.chat(model='command-r-plus', message=q)\n",
        "print(\"Without context:\" )\n",
        "print(response.text)\n",
        "augmented_prompt, source_knowledge = augment_prompt(q, model=model, index=index)\n",
        "response_with_context = co.chat(model='command-r-plus', message=augmented_prompt)\n",
        "print(\"With context:\")\n",
        "print(response_with_context.text)\n",
        "print(\"\\nSource knowledge used:\")\n",
        "print(source_knowledge)\n",
        "print(\"\\n\" + \"-\" * 80 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29MQEYUeCYMN",
        "outputId": "bca60447-1574-454b-dd06-6c703db46428"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Query: How much will Hewlett-Packard pay for Synstar?\n",
            "--------------------------------------------------------------------------------\n",
            "Without context:\n",
            "Hewlett-Packard will pay $420 million for Synstar.\n",
            "With context:\n",
            "Hewlett-Packard will pay $297 million for Synstar.\n",
            "\n",
            "Source knowledge used:\n",
            "hp to buy synstar hewlett-packard will pay \\$297 million for the british company.\n",
            "\n",
            "just all of it that isn't dell. delightful dell the company's results show that it's not grim all over tech world.\n",
            "\n",
            "microsoft has 40-50 billion in the bank.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q=q2\n",
        "print(\"-\" * 80)\n",
        "print(f\"Query: {q}\")\n",
        "print(\"-\" * 80)\n",
        "response = co.chat(model='command-r-plus', message=q)\n",
        "print(\"Without context:\" )\n",
        "print(response.text)\n",
        "augmented_prompt, source_knowledge = augment_prompt(q, model=model, index=index)\n",
        "response_with_context = co.chat(model='command-r-plus', message=augmented_prompt)\n",
        "print(\"With context:\")\n",
        "print(response_with_context.text)\n",
        "print(\"\\nSource knowledge used:\")\n",
        "print(source_knowledge)\n",
        "print(\"\\n\" + \"-\" * 80 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C95jAIsvCheb",
        "outputId": "df5a43e9-da46-4077-a484-0ddfd47684c3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Query: What did the scientists oppose in the Bush administration and how many of them were Nobel Prize winners?\n",
            "--------------------------------------------------------------------------------\n",
            "Without context:\n",
            "The scientists opposed the Bush administration's policies on stem cell research and global warming. Out of the 62 signatories, 20 were Nobel Prize winners.\n",
            "With context:\n",
            "The scientists opposed the Bush administration's use of scientific advice. There were 48 Nobel Prize winners among them.\n",
            "\n",
            "Source knowledge used:\n",
            "science, politics collide in election year (ap) ap - with more than 4,000 scientists, including 48 nobel prize winners, having signed a statement opposing the bush administration's use of scientific advice, this election year is seeing a new development in the uneasy relationship between science and politics.\n",
            "\n",
            "\\\\are you paying attention bush administration?\\\\\n",
            "\n",
            "that's about the state of knowledge for scientists who ponder the question of our planet's rarity.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q=q3\n",
        "print(\"-\" * 80)\n",
        "print(f\"Query: {q}\")\n",
        "print(\"-\" * 80)\n",
        "response = co.chat(model='command-r-plus', message=q)\n",
        "print(\"Without context:\" )\n",
        "print(response.text)\n",
        "augmented_prompt, source_knowledge = augment_prompt(q, model=model, index=index)\n",
        "response_with_context = co.chat(model='command-r-plus', message=augmented_prompt)\n",
        "print(\"With context:\")\n",
        "print(response_with_context.text)\n",
        "print(\"\\nSource knowledge used:\")\n",
        "print(source_knowledge)\n",
        "print(\"\\n\" + \"-\" * 80 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm5B2u12Ci6m",
        "outputId": "176e63a2-f3d1-460f-b1a0-395c3a7af625"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Query: What was the annual base salary of Danny Bazil Riley when he started working as the general manager at a commercial real estate firm?\n",
            "--------------------------------------------------------------------------------\n",
            "Without context:\n",
            "$120,000\n",
            "With context:\n",
            "Danny Bazil Riley's annual base salary when he started working as the general manager at a commercial real estate firm was $70,000.\n",
            "\n",
            "Source knowledge used:\n",
            "safety net (forbes.com) forbes.com - after earning a ph.d. in sociology, danny bazil riley started to work as the general manager at a commercial real estate firm at an annual base salary of  #36;70,000. but, at 32, \"buying insurance was the furthest thing from my mind,\" says riley.\n",
            "\n",
            "making your insurer pay if hurricane charley blows your house down, how can you make your insurance company pay?\n",
            "\n",
            "not to worry: you can find a financial planner for every specialized need\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}